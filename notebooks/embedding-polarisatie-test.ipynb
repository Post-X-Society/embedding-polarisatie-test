{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6clYP9F47wc"
      },
      "source": [
        "# NLP Polarisatie Zelftest obv embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy7BO5SO5BaX"
      },
      "source": [
        "## Mount G-Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvAOEPmo468k"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/My Drive/Post-X/NLPtool/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kcaq0Cxo-V4x"
      },
      "source": [
        "## Get Politician Twitter Handles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFysSRHA-Z1f"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "worksheet = gc.open('politician-twitter-handles').sheet1\n",
        "\n",
        "# get_all_values gives a list of rows.\n",
        "rows = worksheet.get_all_values()\n",
        "\n",
        "# Convert to a DataFrame and render.\n",
        "import pandas as pd\n",
        "df = pd.DataFrame.from_records(rows)\n",
        "\n",
        "# Convert to lists\n",
        "politicians = df.loc[1:, 0]\n",
        "twitter_handles = df.loc[1:, 1]\n",
        "party_acros = df.loc[1:,2]\n",
        "party_names = df.loc[1:,3]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_9CyiZU13Wr"
      },
      "source": [
        "## Get Tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkrFQN-515G5"
      },
      "outputs": [],
      "source": [
        "import tweepy\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "# Twitter API credentials\n",
        "consumer_key = \"\"\n",
        "consumer_secret = \"\"\n",
        "access_key = \"\"\n",
        "access_secret = \"\"\n",
        "\n",
        "# Authorization to consumer key and consumer secret\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "\n",
        "# Access to user's access key and access secret\n",
        "auth.set_access_token(access_key, access_secret)\n",
        "\n",
        "# Calling API\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "# Open/Create a file to append data\n",
        "path = '/content/drive/My Drive/Post-X/NLPtool/data/'\n",
        "csvFile = open(path+datetime.today().strftime('%Y%m%d')+'-raw-tweets.csv', 'a')\n",
        "\n",
        "# Use csv Writer\n",
        "csvWriter = csv.writer(csvFile)\n",
        "\n",
        "# Write header\n",
        "csvWriter.writerow(['politician_name','party_acro','twitter_handle','created_at', 'id_str', 'in_reply_to_status_id_str', 'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'retweeted_status', 'retweet_count', 'full_text'])\n",
        "\n",
        "# Get Politician Tweets function\n",
        "def get_politician_tweet(username, number_of_tweets, politician_name, party_acro):\n",
        "  for tweet in tweepy.Cursor(api.user_timeline, screen_name=username, tweet_mode=\"extended\").items(number_of_tweets):\n",
        "      retweeted_status = hasattr(tweet, 'retweeted_status')\n",
        "      csvWriter.writerow([politician_name, party_acro, username, tweet.created_at, tweet.id_str,\n",
        "                          tweet.in_reply_to_status_id_str, tweet.in_reply_to_user_id_str,\n",
        "                          tweet.in_reply_to_screen_name, retweeted_status,\n",
        "                          tweet.retweet_count, tweet.full_text.encode('utf-8')])\n",
        "\n",
        "# Get Politicians and get 500 tweets for each of them\n",
        "for politician_name, twitter_handle, party_acro in zip(politicians, twitter_handles, party_acros):\n",
        "     get_politician_tweet(twitter_handle, 500, politician_name, party_acro)\n",
        "\n",
        "print(\"Tweets fetched and stored in csv\")\n",
        "\n",
        "csvFile.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Krq_iBMjC9hg"
      },
      "source": [
        "## Get Tweets - Depreciated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfcEz6EhC_1i"
      },
      "outputs": [],
      "source": [
        "consumer_key = \"\"\n",
        "consumer_secret = \"\"\n",
        "_access_token = \"\"\n",
        "_access_token_secret = \"\"\n",
        "\n",
        "import tweepy\n",
        "from tweepy.auth import OAuthHandler\n",
        "import numpy as np\n",
        "\n",
        "def get_politican_tweet(username, number_of_tweets):\n",
        "     auth = OAuthHandler(consumer_key, consumer_secret)\n",
        "     auth.set_access_token(_access_token, _access_token_secret)\n",
        "     # Calling api\n",
        "     api = tweepy.API(auth)\n",
        "     tweets = api.user_timeline(screen_name=username, count = number_of_tweets)\n",
        "     tmp=[]\n",
        "     tweets_for_csv = [tweet.text for tweet in tweets] # CSV file created\n",
        "     for j in tweets_for_csv:\n",
        "         # Appending tweets to the empty array tmp\n",
        "         tmp.append(j)\n",
        "     return tmp\n",
        "\n",
        "final_df = pd.DataFrame()\n",
        "\n",
        "for name, twitterhandle, partyacro in zip(politicians, twitterhandles, partyacros):\n",
        "     tweet = get_politican_tweet(twitterhandle, 200)\n",
        "     temp_df = pd.DataFrame({'tweet':tweet})\n",
        "     temp_df['politican'] = name\n",
        "     temp_df['partyacro'] = partyacro\n",
        "     final_df = final_df.append(temp_df)\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "path = '/content/drive/My Drive/Post-X/NLPtool/'\n",
        "\n",
        "with open(path+'data/'+datetime.today().strftime('%Y%m%d')+'-raw-tweets.csv', 'w', encoding = 'utf-8-sig') as f:\n",
        "  final_df.to_csv(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJjkMOdQqgDs"
      },
      "source": [
        "## Turn CSV into Pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfx9HESlqjnb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Specify the file path or name\n",
        "file = path+'/data/20230529-raw-tweets.csv'\n",
        "\n",
        "# Read the csv file using pandas\n",
        "df = pd.read_csv(file)\n",
        "\n",
        "# Print the DataFrame to verify the result\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New Embeddings Code"
      ],
      "metadata": {
        "id": "isp_G_AknZBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Download Dutch stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize the stemmer\n",
        "stemmer = SnowballStemmer('dutch')\n",
        "\n",
        "# Load Dutch stopwords\n",
        "stop_words = set(stopwords.words('dutch'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Remove stopwords and stem\n",
        "    tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply the preprocessing to the 'full_text' column\n",
        "df['processed_text'] = df['full_text'].apply(preprocess_text)\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "9ufbxPQynby0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "if os.path.exists(path+'labeling.csv'):\n",
        "    df = pd.read_csv(path+'labeling.csv')\n",
        "else:\n",
        "    print('No existing labeling file found.')\n",
        "\n",
        "# Define the widgets\n",
        "start_idx = widgets.IntText(value=0, description='Start Index:', continuous_update=False)\n",
        "button_next = widgets.Button(description=\"Next\")\n",
        "button_prev = widgets.Button(description=\"Back\")\n",
        "button_save = widgets.Button(description=\"Save Progress\")\n",
        "extreme_words = widgets.Checkbox(value=False, description='Extreme Words')\n",
        "dichotomous_lang = widgets.Checkbox(value=False, description='Dichotomous Language')\n",
        "emotional_appeals = widgets.Checkbox(value=False, description='Emotional Appeals')\n",
        "dehumanizing_lang = widgets.Checkbox(value=False, description='Dehumanizing Language')\n",
        "negative_assumptions = widgets.Checkbox(value=False, description='Negative Assumptions')\n",
        "ignoring_counterarguments = widgets.Checkbox(value=False, description='Ignoring Counterarguments')\n",
        "overgeneralization = widgets.Checkbox(value=False, description='Overgeneralization')\n",
        "unverifiable_claims = widgets.Checkbox(value=False, description='Unverifiable Claims')\n",
        "output = widgets.Output()\n",
        "\n",
        "# Set the current tweet index\n",
        "tweet_idx = start_idx.value\n",
        "\n",
        "# Define the update function\n",
        "def update(change):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        print(f'Tweet {tweet_idx+1}/{len(df)}: {df.loc[tweet_idx, \"full_text\"]}')\n",
        "\n",
        "# Define the button click event handlers\n",
        "def on_button_next_clicked(b):\n",
        "    global tweet_idx\n",
        "    if tweet_idx < len(df)-1:\n",
        "        tweet_idx += 1\n",
        "    update(None)\n",
        "\n",
        "def on_button_prev_clicked(b):\n",
        "    global tweet_idx\n",
        "    if tweet_idx > 0:\n",
        "        tweet_idx -= 1\n",
        "    update(None)\n",
        "\n",
        "def on_button_save_clicked(b):\n",
        "    df.to_csv(path+'labeling.csv', index=False)\n",
        "    print(\"Progress saved.\")\n",
        "\n",
        "# Attach the event handlers to the buttons\n",
        "button_next.on_click(on_button_next_clicked)\n",
        "button_prev.on_click(on_button_prev_clicked)\n",
        "button_save.on_click(on_button_save_clicked)\n",
        "\n",
        "# Display the widgets\n",
        "display(start_idx, button_next, button_prev, button_save,\n",
        "        extreme_words, dichotomous_lang, emotional_appeals,\n",
        "        dehumanizing_lang, negative_assumptions, ignoring_counterarguments,\n",
        "        overgeneralization, unverifiable_claims, output)\n",
        "\n",
        "# Trigger the initial update\n",
        "update(None)\n"
      ],
      "metadata": {
        "id": "xga8RRuY1TQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional code"
      ],
      "metadata": {
        "id": "0572bb9by8hs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVeCLj_9rfaq"
      },
      "source": [
        "## Remove URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bRLuYVXsST6"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Function to remove URLs from a string\n",
        "def remove_url(text):\n",
        "    return re.sub(r'http\\S+|www.\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "# Apply the function to the 'full_text' column\n",
        "df['full_text'] = df['full_text'].apply(remove_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpOLebzu2h79"
      },
      "source": [
        "## Calculate readability\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmMYvI3G2hMG"
      },
      "outputs": [],
      "source": [
        "!pip install textstat\n",
        "\n",
        "import textstat\n",
        "\n",
        "# Set the language to Dutch. textstat supports Dutch since version 0.7.0.\n",
        "textstat.set_lang(\"nl\")\n",
        "\n",
        "# Apply the function to the 'full_text' column and store the results in a new column\n",
        "df['flesch_reading_ease'] = df['full_text'].apply(textstat.flesch_reading_ease)\n",
        "\n",
        "# Print the DataFrame to verify the result\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYLXLqOp5H6R"
      },
      "source": [
        "## Create histogram of readability scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqFno8Fz5KLW"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate histogram\n",
        "for party_acro, party_name in zip(party_acros, party_names):\n",
        "  sns.histplot(df[df['party_acro']==party_acro].flesch_reading_ease, kde=True, label = party_name)\n",
        "\n",
        "plt.title('Readability Scores')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.xlim(-10,110)\n",
        "\n",
        "path = '/content/drive/My Drive/Post-X/NLPtool/'\n",
        "plt.savefig(path + '/output/20230529_readability_scores')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUDOUgjo9BXf"
      },
      "source": [
        "## Word Cloud Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG8_z-AE9DDR"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "def remove_hex_codes(text):\n",
        "    return re.sub(r'\\\\x[a-fA-F0-9]{2,}', '', text)\n",
        "\n",
        "# Apply this function to the 'full_text' column in the DataFrame\n",
        "df['cleaned_text'] = df['full_text'].apply(remove_hex_codes)\n",
        "\n",
        "def remove_byte_indicator(text):\n",
        "    return text.lstrip(\"b'\").rstrip(\"'\")\n",
        "\n",
        "# Apply the function to the 'cleaned_text' column\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(remove_byte_indicator)\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Define Dutch stopwords\n",
        "dutch_stopwords = set(stopwords.words('dutch'))\n",
        "\n",
        "def remove_punctuation_and_stopwords(text):\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Lowercase the words\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove stopwords\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in dutch_stopwords]\n",
        "\n",
        "    # Reconstruct the sentence\n",
        "    text = ' '.join(words)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply the function to the 'cleaned_text' column\n",
        "df['cleaned_text'] = df['cleaned_text'].apply(remove_punctuation_and_stopwords)\n",
        "\n",
        "# Create a word cloud for each politician\n",
        "for politician in politicians:\n",
        "    plt.figure(figsize=(10,8))\n",
        "    plt.title(politician)\n",
        "\n",
        "    # Get all tweets from this politician\n",
        "    tweets = df[(df['politician_name'] == politician) & (df['retweeted_status'] != True)]['cleaned_text'].values\n",
        "\n",
        "    # Create one big string from all tweets\n",
        "    text = ' '.join(tweets)\n",
        "\n",
        "    # Create and generate a word cloud image\n",
        "    wordcloud = WordCloud(background_color=\"white\").generate(text)\n",
        "\n",
        "    # Display the generated image\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    plt.savefig(path + '/output/20230529_'+politician+' wordcloud')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auxv2LlHDl-0"
      },
      "source": [
        "## Analyze dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcsHFxCdEJ3d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming that df is your DataFrame and 'created_at' is the column with the tweet timestamp\n",
        "\n",
        "# Convert 'created_at' column to datetime if it's not\n",
        "df['created_at'] = pd.to_datetime(df['created_at'])\n",
        "\n",
        "# Get the number of tweets per politician (not a retweet)\n",
        "tweet_counts = df[df['retweeted_status'] != True]['politician_name'].value_counts()\n",
        "\n",
        "# Get the number of retweets per politician\n",
        "retweet_counts = df[df['retweeted_status'] == True]['politician_name'].value_counts()\n",
        "\n",
        "# Get the date of the oldest tweet per politician\n",
        "oldest_tweets = df_no_retweets.groupby('politician_name')['created_at'].min()\n",
        "\n",
        "# Create a new DataFrame for the plot\n",
        "plot_data = pd.DataFrame({\n",
        "    'Number_of_tweets': tweet_counts,\n",
        "    'Number_of_retweets': retweet_counts,\n",
        "    'Oldest_tweet': oldest_tweets\n",
        "}).reset_index()\n",
        "\n",
        "# Create a bar plot for the number of tweets\n",
        "fig, ax1 = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "ax1.bar(plot_data['index'], plot_data['Number_of_tweets'], color='blue', label='Tweets')\n",
        "ax1.bar(plot_data['index'], plot_data['Number_of_retweets'], color='green', bottom=plot_data['Number_of_tweets'], label='Retweets')\n",
        "ax1.set_ylabel('Number of Tweets', color='blue')\n",
        "ax1.set_xlabel('Politician Name')\n",
        "ax1.tick_params(axis='y', labelcolor='blue')\n",
        "\n",
        "# Explicitly set the rotation of the x-axis labels on the ax1 object\n",
        "for label in ax1.get_xticklabels():\n",
        "    label.set_rotation(90)\n",
        "\n",
        "# Create a second y-axis for the date of the oldest tweet\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(plot_data['index'], plot_data['Oldest_tweet'], color='red')\n",
        "ax2.set_ylabel('Date of Oldest Tweet', color='red')\n",
        "ax2.tick_params(axis='y', labelcolor='red')\n",
        "\n",
        "# Add a legend\n",
        "ax1.legend()\n",
        "\n",
        "plt.title('Number of Tweets, Retweets, and Date of Oldest Tweet per Politician')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6uV3WhuG_Tl"
      },
      "source": [
        "## Stemming and Lemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IH-W_9ejG-zq"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download nl_core_news_lg\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize the Dutch stemmer\n",
        "stemmer = SnowballStemmer(\"dutch\")\n",
        "\n",
        "# Load SpaCy's Dutch model\n",
        "nlp = spacy.load('nl_core_news_lg')\n",
        "\n",
        "# Filter out retweets\n",
        "df_no_retweets = df[df['retweeted_status'] != True]\n",
        "\n",
        "# Tokenize and stem each tweet\n",
        "df_no_retweets['tokens'] = df_no_retweets['cleaned_text'].apply(word_tokenize)\n",
        "df_no_retweets['stemmed'] = df_no_retweets['tokens'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
        "\n",
        "# Lemmatize each tweet using SpaCy\n",
        "df_no_retweets['lemmatized'] = df_no_retweets['cleaned_text'].apply(lambda text: [token.lemma_ for token in nlp(text)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBA6gLecKfvb"
      },
      "source": [
        "## Store in CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeYnCA2OKKgQ"
      },
      "outputs": [],
      "source": [
        "df_no_retweets.to_csv(path+'/data/20230529-processed-no-retweets.csv', index=False)\n",
        "\n",
        "df_no_retweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbfMdr5AM7kp"
      },
      "source": [
        "## Load CSV to dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9imLEqKM9b0"
      },
      "outputs": [],
      "source": [
        "# Specify the file path or name\n",
        "file = path+'/data/20230529-processed-no-retweets.csv'\n",
        "\n",
        "# Read the csv file using pandas\n",
        "df_no_retweets = pd.read_csv(file)\n",
        "\n",
        "import ast\n",
        "\n",
        "# Function to convert string to list\n",
        "def convert_string_to_list(s):\n",
        "    try:\n",
        "        return ast.literal_eval(s)\n",
        "    except (ValueError, SyntaxError):\n",
        "        # In case of any exception, return an empty list\n",
        "        return []\n",
        "\n",
        "# Apply the function to the 'lemmatized' column\n",
        "df_no_retweets['lemmatized'] = df_no_retweets['lemmatized'].apply(convert_string_to_list)\n",
        "\n",
        "# Print the DataFrame to verify the result\n",
        "print(df_no_retweets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYNb_y4dMKFc"
      },
      "source": [
        "## Drop unnecessary columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_1mhF3EMNNC"
      },
      "outputs": [],
      "source": [
        "df_no_retweets = df_no_retweets.drop(['full_text', 'tokens'], axis=1)\n",
        "df_no_retweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sH2U2a5IMnKi"
      },
      "source": [
        "## Sentiment Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVVeh-E8Mo9u"
      },
      "outputs": [],
      "source": [
        "!pip install pattern\n",
        "\n",
        "from pattern.nl import sentiment\n",
        "\n",
        "def calculate_sentiment(text):\n",
        "    sentiment_score, _ = sentiment(text)  # Unpack the tuple\n",
        "    return sentiment_score\n",
        "\n",
        "# Now, apply this function to each cleaned_retweet\n",
        "df_no_retweets['sentiment_score'] = df_no_retweets['cleaned_text'].apply(calculate_sentiment)\n",
        "\n",
        "df_no_retweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DETsC3sOqtL"
      },
      "source": [
        "## Analyze sentiment scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R5oRnHiOslT"
      },
      "outputs": [],
      "source": [
        "def sentiment_grouping(sentiment_compound):\n",
        "    if sentiment_compound >= 0.05:\n",
        "        return 'Positive'\n",
        "    elif (sentiment_compound > -0.05 and sentiment_compound < 0.05):\n",
        "        return 'Neutral'\n",
        "    elif sentiment_compound <= -0.05:\n",
        "        return 'Negative'\n",
        "\n",
        "df_no_retweets['sentiment'] = df_no_retweets['sentiment_score'].apply(sentiment_grouping)\n",
        "df_no_retweets.groupby(['politician_name', 'sentiment']).size()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWplHkP5yD6i"
      },
      "source": [
        "## Visualize sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7Com5WvyF_x"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Count the sentiment categories for each politician\n",
        "sentiment_counts = df_no_retweets.groupby('politician_name')['sentiment'].value_counts()\n",
        "\n",
        "# Get the list of politicians\n",
        "politicians = df_no_retweets['politician_name'].unique()\n",
        "\n",
        "# Calculate the number of rows needed for the subplots\n",
        "num_rows = int(np.ceil(len(politicians) / 4))\n",
        "\n",
        "# Create the subplots\n",
        "fig, axs = plt.subplots(num_rows, 4, figsize=(20, 6*num_rows))\n",
        "\n",
        "# Flatten the axis array for easier indexing\n",
        "axs = axs.flatten()\n",
        "\n",
        "# Define a color map for the sentiment categories\n",
        "color_map = {'Positive': 'green', 'Neutral': 'blue', 'Negative': 'red'}\n",
        "\n",
        "for i, politician in enumerate(politicians):\n",
        "    # If the politician has tweets in the DataFrame\n",
        "    if politician in sentiment_counts:\n",
        "        # Get the sentiment counts for this politician\n",
        "        counts = sentiment_counts[politician]\n",
        "\n",
        "        # Create a list of colors for the pie chart slices based on the sentiment categories\n",
        "        colors = [color_map[label] for label in counts.index]\n",
        "\n",
        "        # Create a pie chart in the i-th subplot\n",
        "        axs[i].pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=140, colors=colors)\n",
        "        axs[i].set_title(f'Sentiment Distribution for {politician}')\n",
        "\n",
        "# Remove empty subplots\n",
        "for i in range(len(politicians), len(axs)):\n",
        "    fig.delaxes(axs[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.savefig(path + '/output/20230529-sentiment-charts')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cShq57ngz_Mg"
      },
      "source": [
        "## Bag of Words - Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Y3QKKAK0CCS"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize a CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# For each politician, generate a bag of words\n",
        "for politician in df_no_retweets['politician_name'].unique():\n",
        "    # Get the tweets for this politician\n",
        "    tweets = df_no_retweets[df_no_retweets['politician_name'] == politician]['lemmatized']\n",
        "\n",
        "    # Convert list of tokens back to sentences\n",
        "    sentences = tweets.apply(' '.join)\n",
        "\n",
        "    # Generate the bag of words\n",
        "    bow = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    # Create a DataFrame for better visualization\n",
        "    bow_df = pd.DataFrame(bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "    # Show the bag of words\n",
        "    print(f'Bag of Words for {politician}:')\n",
        "    print(bow_df)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6zIqpfZ3nZh"
      },
      "source": [
        "## BoW Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aZdadfu3ow7"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize a CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Create a dictionary to hold the BoW for each politician\n",
        "politician_bow = {}\n",
        "\n",
        "for politician in df_no_retweets['politician_name'].unique():\n",
        "    # Get the lemmatized tweets for this politician\n",
        "    tweets = df_no_retweets[df_no_retweets['politician_name'] == politician]['lemmatized']\n",
        "\n",
        "    # Convert list of lemmas back to sentences\n",
        "    sentences = tweets.apply(' '.join)\n",
        "\n",
        "    # Generate the bag of words\n",
        "    bow = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    # Save the feature names (words) in the dictionary\n",
        "    politician_bow[politician] = set(vectorizer.get_feature_names_out())\n",
        "\n",
        "# Function to count the number of words that appear in the BoW\n",
        "def count_bow_words(tokens, bow):\n",
        "    return sum(token in bow for token in tokens)\n",
        "\n",
        "# Create a new column for each politician\n",
        "for politician, bow in politician_bow.items():\n",
        "    df_no_retweets[f'{politician}_bow_count'] = df_no_retweets['lemmatized'].apply(count_bow_words, bow=bow)\n",
        "\n",
        "df_no_retweets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XstK7NAf4Ss1"
      },
      "source": [
        "## Show BoW Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcRNmRL55rFP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Extract the columns that end with \"_bow_count\"\n",
        "bow_columns = [col for col in df_no_retweets.columns if col.endswith('_bow_count')]\n",
        "\n",
        "# Pre-calculate filters\n",
        "filtered_data = {politician: df_no_retweets[df_no_retweets['politician_name'] == politician] for politician in df_no_retweets['politician_name'].unique()}\n",
        "\n",
        "# Initialize an empty DataFrame to store the results\n",
        "summary_pd = pd.DataFrame(columns=bow_columns, index=filtered_data.keys())\n",
        "\n",
        "# Calculate sum of bow counts\n",
        "for politician, data in filtered_data.items():\n",
        "    summary_pd.loc[politician] = data[bow_columns].sum().to_dict()\n",
        "\n",
        "print(summary_pd)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r097EIIZ5qMN"
      },
      "source": [
        "## Visualize the interactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfjT2mZG5sKd"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Convert all columns to numeric type\n",
        "summary_pd = summary_pd.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Create a heatmap\n",
        "sns.heatmap(summary_pd, cmap=\"YlGnBu\")\n",
        "\n",
        "plt.title('Politician Interaction Heatmap based on BoW')\n",
        "plt.show()\n",
        "plt.savefig(path + '/output/20230529-bow-heatmap')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY_w3IzP6cwF"
      },
      "source": [
        "## No common words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1D5UkZYA7gDs"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize a CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Create a dictionary to store the BoWs\n",
        "bows = {}\n",
        "\n",
        "# For each politician, generate a bag of words\n",
        "for politician in df_no_retweets['politician_name'].unique():\n",
        "    # Get the tweets for this politician\n",
        "    tweets = df_no_retweets[df_no_retweets['politician_name'] == politician]['lemmatized']\n",
        "\n",
        "    # Convert list of tokens back to sentences\n",
        "    sentences = tweets.apply(' '.join)\n",
        "\n",
        "    # Generate the bag of words\n",
        "    bow = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    # Create a DataFrame for better visualization\n",
        "    bow_df = pd.DataFrame(bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "    # Store the bag of words\n",
        "    bows[politician] = bow_df\n",
        "\n",
        "# Function to remove common words across all BoWs\n",
        "def remove_common_words(bows):\n",
        "    # Create a set with all words\n",
        "    all_words = set().union(*[set(bow.columns) for bow in bows.values()])\n",
        "\n",
        "    # Find common words\n",
        "    common_words = {word for word in all_words if all(word in bow.columns for bow in bows.values())}\n",
        "\n",
        "    # Remove common words from each BoW\n",
        "    for bow in bows.values():\n",
        "        bow.drop(columns=list(common_words), inplace=True, errors='ignore')\n",
        "    return bows\n",
        "\n",
        "# Remove common words\n",
        "bows = remove_common_words(bows)\n",
        "\n",
        "# Show the bag of words\n",
        "for politician, bow in bows.items():\n",
        "    print(f'Bag of Words for {politician}:')\n",
        "    print(bow)\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hc8yTs1H7pxj"
      },
      "outputs": [],
      "source": [
        "# Re-calculate _bow_count values\n",
        "for politician, bow_df in bows.items():\n",
        "    unique_words = bow_df.columns\n",
        "    df_no_retweets[f'{politician}_bow_count'] = df_no_retweets['lemmatized'].apply(lambda words: len([word for word in words if word in unique_words]))\n",
        "\n",
        "# Prepare the summary dataframe\n",
        "summary_df = pd.DataFrame(index=df_no_retweets['politician_name'].unique())\n",
        "bow_columns = [col for col in df_no_retweets.columns if col.endswith('_bow_count')]\n",
        "\n",
        "for politician in summary_df.index:\n",
        "    for bow_column in bow_columns:\n",
        "        total_count = df_no_retweets[df_no_retweets['politician_name'] == politician][bow_column].sum()\n",
        "        summary_df.loc[politician, bow_column] = total_count\n",
        "\n",
        "# Ensure that the data is of numeric type\n",
        "summary_df = summary_df.astype(float)\n",
        "\n",
        "# Generate heatmap\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(summary_df, cmap=\"YlGnBu\")\n",
        "plt.title('Politician Interaction Heatmap based on BoW w/o common words')\n",
        "plt.show()\n",
        "plt.savefig(path + '/output/20230529-bow-heatmap-no-common')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vluNgkch90eU"
      },
      "source": [
        "## TF IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kXeLegm92IT"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import joblib\n",
        "\n",
        "# Initialize a TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Convert list of lemmatized tokens back to sentences for each tweet\n",
        "sentences = df_no_retweets['lemmatized'].apply(' '.join)\n",
        "\n",
        "# Generate the TF-IDF feature matrix\n",
        "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Save the fitted TfidfVectorizer to a file\n",
        "joblib.dump(vectorizer, path+'output/tfidf_vectorizer.pkl')\n",
        "\n",
        "print(tfidf_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOG-FjS4-QrW"
      },
      "source": [
        "## Random Forest model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urLYDSsL-SfV"
      },
      "outputs": [],
      "source": [
        "!pip install joblib\n",
        "\n",
        "import joblib\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Add readability and sentiment scores to our feature matrix\n",
        "tfidf_df['flesch_reading_ease'] = df_no_retweets['flesch_reading_ease']\n",
        "tfidf_df['sentiment'] = df_no_retweets['sentiment_score']\n",
        "\n",
        "# Create a label encoder for the target variable (politician_name)\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df_no_retweets['politician_name'])\n",
        "\n",
        "# Perform a train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_df, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize a RandomForestClassifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Save the model\n",
        "joblib.dump(clf, path+'output/230529_rf_model.pkl')\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Print a classification report\n",
        "print(classification_report(y_test, y_pred, target_names=le.classes_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSekA1JS_dY7"
      },
      "source": [
        "## Predict a tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2aiw2Zjs_fhm"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "\n",
        "!python -m spacy download nl_core_news_lg\n",
        "import spacy\n",
        "from spacy.lemmatizer import Lemmatizer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define Dutch stopwords\n",
        "dutch_stopwords = set(stopwords.words('dutch'))\n",
        "\n",
        "# Initialize stemmer\n",
        "stemmer = SnowballStemmer('dutch')\n",
        "\n",
        "# Initiatlize lemmatizer\n",
        "lemmatizer = Lemmatizer()\n",
        "\n",
        "def remove_urls(text):\n",
        "    return re.sub(r'http\\S+|www.\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "def remove_hex_codes(text):\n",
        "    return re.sub(r'\\\\x[a-fA-F0-9]{2,}', '', text)\n",
        "\n",
        "def remove_byte_indicator(text):\n",
        "    return text.lstrip(\"b'\").rstrip(\"'\")\n",
        "\n",
        "def remove_punctuation_and_stopwords(text):\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Lowercase the words\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove stopwords\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in dutch_stopwords]\n",
        "\n",
        "    # Reconstruct the sentence\n",
        "    text = ' '.join(words)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Load the model\n",
        "rf_model = joblib.load(path + 'output/230529_rf_model.pkl')\n",
        "\n",
        "# Load the TfidfVectorizer\n",
        "tfidf_vectorizer = joblib.load(path + 'output/tfidf_vectorizer.pkl')\n",
        "\n",
        "# Function to preprocess the tweet\n",
        "def preprocess_tweet(tweet):\n",
        "    # Remove URLs, hexcodes and byte indicator\n",
        "    tweet = remove_urls(tweet)\n",
        "    tweet = remove_hex_codes(tweet)\n",
        "    tweet = remove_byte_indicator(tweet)\n",
        "\n",
        "    # Remove punctuation and stopwords\n",
        "    tweet = remove_punctuation_and_stopwords(tweet)\n",
        "\n",
        "    # Tokenize the tweet\n",
        "    tokens = word_tokenize(tweet)\n",
        "\n",
        "    # Lemmatize, stem, and remove stop words\n",
        "    preprocessed_tokens = [stemmer.stem(lemmatizer.lemmatize(token)) for token in tokens]\n",
        "\n",
        "    # Join tokens back into a sentence\n",
        "    preprocessed_tweet = ' '.join(preprocessed_tokens)\n",
        "\n",
        "    return preprocessed_tweet\n",
        "\n",
        "# Function to vectorize the tweet\n",
        "def vectorize_tweet(tweet):\n",
        "    # Preprocess the tweet\n",
        "    preprocessed_tweet = preprocess_tweet(tweet)\n",
        "\n",
        "    # Transform the preprocessed tweet into TF-IDF features\n",
        "    tfidf_vector = tfidf_vectorizer.transform([preprocessed_tweet])\n",
        "\n",
        "    return tfidf_vector\n",
        "\n",
        "# Function to predict the politician\n",
        "def predict_politician(tweet):\n",
        "    # Vectorize the tweet\n",
        "    tweet_vector = vectorize_tweet(tweet)\n",
        "\n",
        "    # Predict the politician using the Random Forest model\n",
        "    prediction = rf_model.predict(tweet_vector)\n",
        "\n",
        "    return prediction[0]\n",
        "\n",
        "# Ask the user to enter a tweet\n",
        "tweet = input(\"Please enter a tweet: \")\n",
        "\n",
        "# Predict the politician and print the result\n",
        "predicted_politician = predict_politician(tweet)\n",
        "print(f\"The tweet is predicted to be written by: {predicted_politician}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f0gD8Od5zz9"
      },
      "source": [
        "## Unmount G-Drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEW9ZNUW527U"
      },
      "outputs": [],
      "source": [
        "drive.flush_and_unmount()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
